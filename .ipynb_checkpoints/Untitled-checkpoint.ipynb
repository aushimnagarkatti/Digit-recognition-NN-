{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpynet.layer import Dense, ELU, ReLU, SoftmaxCrossEntropy\n",
    "from numpynet.function import Softmax\n",
    "from numpynet.utils import Dataloader, one_hot_encoding, load_MNIST, save_csv\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntType = np.int64\n",
    "FloatType = np.float64\n",
    "\n",
    "class Model(object):\n",
    "    \"\"\"Model Your Deep Neural Network\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        \"\"\"__init__ Constructor\n",
    "\n",
    "        Arguments:\n",
    "            input_dim {IntType or int} -- Number of input dimensions\n",
    "            output_dim {IntType or int} -- Number of classes\n",
    "        \"\"\"\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.loss_fn = SoftmaxCrossEntropy(axis=-1)\n",
    "        self.build_model()\n",
    "        self.accuracy=0\n",
    "       \n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"build_model Build the model using numpynet API\n",
    "        \"\"\"\n",
    "        self.l1=Dense( 784, 256)\n",
    "        self.l2=ELU(1)\n",
    "        self.l3=Dense(256,64)\n",
    "        self.l4=ELU(1)\n",
    "        self.l5=Dense(64,10)\n",
    "        self.l6=ELU(1)\n",
    "        self.l7=SoftmaxCrossEntropy()\n",
    "\n",
    "    def __call__(self, X):\n",
    "        \"\"\"__call__ Forward propogation of the model\n",
    "\n",
    "        Arguments:\n",
    "            X {np.ndarray} -- Input batch\n",
    "           \n",
    "\n",
    "        Returns:\n",
    "            np.ndarray -- The output of the model. \n",
    "                You can return the logits or probits, \n",
    "                which depends on the way how you structure \n",
    "                the code.\n",
    "        \"\"\"\n",
    "        op=self.l1.__call__(X)\n",
    "        op=self.l2.__call__(op)\n",
    "        op=self.l3.__call__(op)\n",
    "        op=self.l4.__call__(op)\n",
    "        op=self.l5.__call__(op)\n",
    "        op=self.l6.__call__(op)    \n",
    "\n",
    "        return op\n",
    "\n",
    "        \n",
    "\n",
    "    def bprop(self, istraining,X,train_Y):\n",
    "        \"\"\"bprop Backward propogation of the model\n",
    "\n",
    "        Arguments:\n",
    "            logits {np.ndarray} -- The logits of the model output, \n",
    "                which means the pre-softmax output, since you need \n",
    "                to pass the logits into SoftmaxCrossEntropy.\n",
    "            labels {np,ndarray} -- True one-hot lables of the input batch.\n",
    "\n",
    "        Keyword Arguments:\n",
    "            istraining {bool} -- If False, only compute the loss. If True, \n",
    "                compute the loss first and propagate the gradients through \n",
    "                each layer. (default: {True})\n",
    "\n",
    "        Returns:\n",
    "            FloatType or float -- The loss of the iteration\n",
    "            Logits\n",
    "        \"\"\"\n",
    "        output=self.__call__(X) #gets the output of last ELU layer\n",
    "        loss=self.l7.__call__(output,train_Y)  #calls cross entropy&softmax, and returns loss\n",
    "        if(istraining==False):\n",
    "            return loss,output\n",
    "        \n",
    "        grad=self.l7.bprop()\n",
    "\n",
    "        #3rd Dense Layer\n",
    "        gradelu=self.l6.bprop()\n",
    "        grad=np.multiply(gradelu,grad)\n",
    "        grad=self.l5.bprop(grad)\n",
    "\n",
    "        #2nd Dense Layer\n",
    "        gradelu=self.l4.bprop()\n",
    "        grad=np.multiply(gradelu,grad)\n",
    "        grad=self.l3.bprop(grad)\n",
    "\n",
    "        #1st Dense Layer\n",
    "        gradelu=self.l2.bprop()\n",
    "        grad=np.multiply(gradelu,grad)\n",
    "        grad=self.l1.bprop(grad)\n",
    "\n",
    "        return loss,output\n",
    "\n",
    "        \n",
    "\n",
    "    def update_parameters(self, lr):\n",
    "        \"\"\"update_parameters Update the parameters for each layer.\n",
    "\n",
    "        Arguments:\n",
    "            lr {FloatType or float} -- The learning rate\n",
    "        \"\"\"\n",
    "        self.l5.update(lr)\n",
    "        self.l3.update(lr)\n",
    "        self.l1.update(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,\n",
    "          train_X,\n",
    "          train_y,\n",
    "          max_epochs=20,\n",
    "          lr=1e-3,\n",
    "          batch_size=16,\n",
    "          metric_fn=accuracy_score,\n",
    "          **kwargs):\n",
    "    \"\"\"train Train the model\n",
    "\n",
    "    Arguments:\n",
    "        model {Model} -- The Model object\n",
    "        train_X {np.ndarray} -- Training dataset\n",
    "        train_y {np.ndarray} -- Training labels\n",
    "        val_X {np.ndarray} -- Validation dataset\n",
    "        val_y {np.ndarray} -- Validation labels\n",
    "\n",
    "    Keyword Arguments:\n",
    "        max_epochs {IntType or int} -- Maximum training expochs (default: {20})\n",
    "        lr {FloatType or float} -- Learning rate (default: {1e-3})\n",
    "        batch_size {IntType or int} -- Size of each mini batch (default: {16})\n",
    "        metric_fn {function} -- Metric function to measure the performance of \n",
    "            the model (default: {accuracy_score})\n",
    "    \"\"\"\n",
    "   \n",
    "    #forward prop is called inside bprop\n",
    "    loss, output=model.bprop(True,train_X,train_y) #loss is Bx1,\n",
    "\n",
    "    #Update weights\n",
    "    model.update_parameters(lr)    \n",
    "\n",
    "    #Calculate accuracy\n",
    "    for i in range(batch_size):\n",
    "        ind=np.where(output[i]==max(output[i]))[0][0]\n",
    "        true_ind=np.where(train_y[i]==1)[0][0]\n",
    "        if(ind==true_ind):\n",
    "            model.accuracy+=1\n",
    "    \n",
    "\n",
    "    #Report training error per instance\n",
    "    train_loss=np.average(loss)/50000 #/49984\n",
    "    #print(\"Training Loss:\",train_loss)\n",
    "    return train_loss\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def inference(model,X, y, batch_size=16, metric_fn=accuracy_score, **kwargs):\n",
    "    \"\"\"inference Run the inference on the given dataset\n",
    "\n",
    "    Arguments:\n",
    "        model {Model} -- The Neural Network model\n",
    "        X {np.ndarray} -- The dataset input\n",
    "        y {np.ndarray} -- The sdataset labels\n",
    "\n",
    "    Keyword Arguments:\n",
    "        metric {function} -- Metric function to measure the performance of the model \n",
    "            (default: {accuracy_score})\n",
    "\n",
    "    Returns:\n",
    "        tuple of (float, float): A tuple of the loss and accuracy\n",
    "    \"\"\"\n",
    "    \n",
    "    #forward prop is called inside bprop\n",
    "    loss, output=model.bprop(False,X,y) #loss is Bx1,\n",
    "\n",
    "\n",
    "    #Calculate accuracy\n",
    "    for i in range(batch_size):\n",
    "        ind=np.where(output[i]==max(output[i]))[0][0]\n",
    "        true_ind=np.where(y[i]==1)[0][0]\n",
    "        if(ind==true_ind):\n",
    "            model.accuracy+=1\n",
    "    \n",
    "\n",
    "    #Report training error per instance\n",
    "    val_loss=np.average(loss)/10000 #/9984\n",
    "    #print(\"Validation Loss:\",val_loss)\n",
    "    return val_loss\n",
    "\n",
    "\n",
    "def predict(model,X):\n",
    "    output=model.__call__(X) #gets the output of last ELU layer\n",
    "    #Calculate accuracy\n",
    "    y_pred=[]\n",
    "    for i in range(len(output)):\n",
    "        ind=np.where(output==max(output[i]))[0][0]\n",
    "        y_pred.append(ind)\n",
    "    return np.array(y_pred)   \n",
    "\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('loading data #####')\n",
    "    train_X, train_y = load_MNIST(path ='dataset/',name=\"train\")\n",
    "    val_X, val_y = load_MNIST(path = 'dataset/', name=\"val\")\n",
    "    test_X = load_MNIST(path = 'dataset/', name=\"test\")\n",
    "    print('loading data complete #####')\n",
    "\n",
    "    # TODO: 1. Build your model\n",
    "    # TODO: 2. Train your model with training dataset and\n",
    "    #       validate it  on the validation dataset\n",
    "    # TODO: 3. Test your trained model on the test dataset\n",
    "    #       you need have at least 95% accuracy on the test dataset to receive full scores\n",
    "\n",
    "    # Your code starts here\n",
    "\n",
    "    #NOTE: WE HAVE PROVIDED A SKELETON FOR THE MAIN FUNCTION. FEEL FREE TO CHANGE IT AS YOU WISH, THIS IS JUST A SUGGESTED FORMAT TO HELP YOU.\n",
    "\n",
    "    batchSize = 16\n",
    "    learningRate = 0.2 \n",
    "    model = Model(input_dim = 784,output_dim = 10)\n",
    "    model.build_model()\n",
    "    print('Model built #####')\n",
    "    t_loss=[]\n",
    "    t_acc=[]\n",
    "    v_loss=[]\n",
    "    v_acc=[]\n",
    "\n",
    "    #50k training samples, 200 epochs --> 1562 batches of size 32 samples/batch\n",
    "    #Start training \n",
    "    for k in range(1,201):\n",
    "\n",
    "        print(\"\\n\\nEpoch\",k)\n",
    "        print(\"********\")\n",
    "        if(k==160):\n",
    "            learningRate=0.02\n",
    "\n",
    "        #Training        \n",
    "        one_hot_train_y = one_hot_encoding(train_y)\n",
    "        train_dataloader = Dataloader(X=train_X, y=one_hot_train_y,batch_size=batchSize)\n",
    "        model.accuracy=0\n",
    "        #for each batch \n",
    "        for i, (features, labels) in enumerate(train_dataloader):\n",
    "            #call model train\n",
    "            #if(i<1562):\n",
    "                train_loss = train(model,features, labels, max_epochs=200, lr=learningRate, batch_size=batchSize, metric_fn=accuracy_score)\n",
    "        t_acc.append(model.accuracy/50000) #/49984)\n",
    "        t_loss.append(train_loss)\n",
    "        print(\"Training Loss:\",train_loss)\n",
    "        print(\"Training Accuracy:\",model.accuracy/50000)\n",
    "\n",
    "        #Validation\n",
    "        model.accuracy=0\n",
    "        one_hot_val_y = one_hot_encoding(val_y)\n",
    "        val_dataloader = Dataloader(X=val_X, y=one_hot_val_y,batch_size=batchSize)\n",
    "        if(k%2==0):\n",
    "            for i, (features, labels) in enumerate(val_dataloader):\n",
    "                #call validation\n",
    "                #if(i<312):\n",
    "                    val_loss= inference(model,features, labels, batch_size = batchSize)\n",
    "                \n",
    "            v_acc.append(model.accuracy/10000) #/9984)\n",
    "            v_loss.append(val_loss) \n",
    "            print(\"Validation Loss:\",val_loss)\n",
    "            print(\"Validation Accuracy:\",model.accuracy/10000)\n",
    "         \n",
    "\n",
    "    print('Training complete #####')\n",
    "    \n",
    "    # Plot of train and val accuracy vs iteration\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.title('Accuracy vs number of iterations')\n",
    "    plt.plot(np.linspace(0,199,200), t_acc, label = 'Train accuracy across iterations')\n",
    "    plt.plot(np.linspace(0,198,100), v_acc, label = 'Val accuracy across iterations')\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot of train and val loss vs iteration\n",
    "    plt.figure(figsize=(10,7))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.title('Loss vs number of iterations')\n",
    "    plt.plot(np.linspace(0,199,200), t_loss, label = 'Train loss across iterations')\n",
    "    plt.plot(np.linspace(0,198,100), v_loss, label = 'Val loss across iteration')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "    # Inference on test dataset without labels\n",
    "\n",
    "    #Implement inference function so that you can return the test prediction output and save it in test_pred. You are allowed to create a different function to generate just the predicted labels.\n",
    "    test_pred = predict(model,test_X)\n",
    "    \n",
    "    save_csv(test_pred)\n",
    "    # Your code ends here\n",
    "\n",
    "    print(\"Validation loss: {0}, Validation Acc: {1}%\".format(v_loss[-1], 100 * v_acc[-1]))\n",
    "    if v_acc[-1] > 0.95:\n",
    "        print(\"Your model is well-trained.\")\n",
    "    else:\n",
    "        print(\"You still need to tune your model\")\n",
    "\n",
    "   \n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
